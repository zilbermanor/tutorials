{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Product2Vec Skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/77/63/a9fa76de8dffe7455304c4ed635be4aa9c0bacef6e0633d87d5f54530c5c/tensorflow-1.13.1-cp36-cp36m-manylinux1_x86_64.whl (92.5MB)\n",
      "\u001b[K    100% |################################| 92.5MB 812kB/s \n",
      "\u001b[?25hCollecting gast>=0.2.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
      "Requirement already satisfied: numpy>=1.13.3 in /conda/lib/python3.6/site-packages (from tensorflow) (1.16.2)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz\n",
      "Collecting keras-applications>=1.0.6 (from tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/90/85/64c82949765cfb246bbdaf5aca2d55f400f792655927a017710a78445def/Keras_Applications-1.0.7-py2.py3-none-any.whl (51kB)\n",
      "\u001b[K    100% |################################| 61kB 26.5MB/s \n",
      "\u001b[?25hCollecting tensorflow-estimator<1.14.0rc0,>=1.13.0 (from tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/48/13f49fc3fa0fdf916aa1419013bb8f2ad09674c275b4046d5ee669a46873/tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367kB)\n",
      "\u001b[K    100% |################################| 368kB 48.0MB/s \n",
      "\u001b[?25hCollecting keras-preprocessing>=1.0.5 (from tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c0/bf/0315ef6a9fd3fc2346e85b0ff1f5f83ca17073f2c31ac719ab2e4da0d4a3/Keras_Preprocessing-1.0.9-py2.py3-none-any.whl (59kB)\n",
      "\u001b[K    100% |################################| 61kB 29.3MB/s \n",
      "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /conda/lib/python3.6/site-packages (from tensorflow) (1.19.0)\n",
      "Collecting astor>=0.6.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/35/6b/11530768cac581a12952a2aad00e1526b89d242d0b9f59534ef6e6a1752f/astor-0.7.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: six>=1.10.0 in /conda/lib/python3.6/site-packages (from tensorflow) (1.12.0)\n",
      "Collecting tensorboard<1.14.0,>=1.13.0 (from tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/39/bdd75b08a6fba41f098b6cb091b9e8c7a80e1b4d679a581a0ccd17b10373/tensorboard-1.13.1-py3-none-any.whl (3.2MB)\n",
      "\u001b[K    100% |################################| 3.2MB 19.7MB/s \n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /conda/lib/python3.6/site-packages (from tensorflow) (3.7.1)\n",
      "Requirement already satisfied: wheel>=0.26 in /conda/lib/python3.6/site-packages (from tensorflow) (0.33.1)\n",
      "Collecting absl-py>=0.1.6 (from tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/da/3f/9b0355080b81b15ba6a9ffcf1f5ea39e307a2778b2f2dc8694724e8abd5b/absl-py-0.7.1.tar.gz (99kB)\n",
      "\u001b[K    100% |################################| 102kB 47.2MB/s \n",
      "\u001b[?25hCollecting h5py (from keras-applications>=1.0.6->tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/30/99/d7d4fbf2d02bb30fb76179911a250074b55b852d34e98dd452a9f394ac06/h5py-2.9.0-cp36-cp36m-manylinux1_x86_64.whl (2.8MB)\n",
      "\u001b[K    100% |################################| 2.8MB 21.3MB/s \n",
      "\u001b[?25hCollecting mock>=2.0.0 (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e6/35/f187bdf23be87092bd0f1200d43d23076cee4d0dec109f195173fd3ebc79/mock-2.0.0-py2.py3-none-any.whl (56kB)\n",
      "\u001b[K    100% |################################| 61kB 24.8MB/s \n",
      "\u001b[?25hCollecting werkzeug>=0.11.15 (from tensorboard<1.14.0,>=1.13.0->tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/79/84f02539cc181cdbf5ff5a41b9f52cae870b6f632767e43ba6ac70132e92/Werkzeug-0.15.2-py2.py3-none-any.whl (328kB)\n",
      "\u001b[K    100% |################################| 337kB 50.9MB/s \n",
      "\u001b[?25hCollecting markdown>=2.6.8 (from tensorboard<1.14.0,>=1.13.0->tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/e4/d8c18f2555add57ff21bf25af36d827145896a07607486cc79a2aea641af/Markdown-3.1-py2.py3-none-any.whl (87kB)\n",
      "\u001b[K    100% |################################| 92kB 49.0MB/s \n",
      "\u001b[?25hRequirement already satisfied: setuptools in /conda/lib/python3.6/site-packages (from protobuf>=3.6.1->tensorflow) (40.8.0)\n",
      "Collecting pbr>=0.11 (from mock>=2.0.0->tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/3e/22d1d35a4b51706ca3590c54359aeb5fa7ea60df46180143a3ea13d45f29/pbr-5.2.0-py2.py3-none-any.whl (107kB)\n",
      "\u001b[K    100% |################################| 112kB 56.4MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: gast, termcolor, absl-py\n",
      "  Running setup.py bdist_wheel for gast ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /igz/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
      "  Running setup.py bdist_wheel for termcolor ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /igz/.cache/pip/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6\n",
      "  Running setup.py bdist_wheel for absl-py ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /igz/.cache/pip/wheels/ee/98/38/46cbcc5a93cfea5492d19c38562691ddb23b940176c14f7b48\n",
      "Successfully built gast termcolor absl-py\n",
      "Installing collected packages: gast, termcolor, h5py, keras-applications, absl-py, pbr, mock, tensorflow-estimator, keras-preprocessing, astor, werkzeug, markdown, tensorboard, tensorflow\n",
      "Successfully installed absl-py-0.7.1 astor-0.7.1 gast-0.2.2 h5py-2.9.0 keras-applications-1.0.7 keras-preprocessing-1.0.9 markdown-3.1 mock-2.0.0 pbr-5.2.0 tensorboard-1.13.1 tensorflow-1.13.1 tensorflow-estimator-1.13.0 termcolor-1.1.0 werkzeug-0.15.2\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create TF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from functools import reduce\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Configurations\n",
    "BASE_DIR = os.getcwd()\n",
    "DATA_DIR = os.path.join(BASE_DIR, 'data/')\n",
    "MODELS_DIR = os.path.join(BASE_DIR, 'models/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Product 2 Vec skipgram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Product2VecSkipGram:\n",
    "    def __init__(self, data, cv_data, batch_size, num_skips, skip_window, vocabulary_size, embedding_size=32,\n",
    "                 num_negative_sampled=64, len_ratio = 0.5, models_dir=MODELS_DIR):\n",
    "        self.data = data\n",
    "        self.cv_data = cv_data\n",
    "        self.data_index = 0\n",
    "        self.batch_size = batch_size\n",
    "        self.num_skips = num_skips\n",
    "        self.skip_window = skip_window\n",
    "        self.embedding_size = embedding_size\n",
    "        self.num_negative_sampled = num_negative_sampled\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        self.len_ratio = len_ratio\n",
    "        self.models_dir = models_dir\n",
    "        assert batch_size % num_skips == 0\n",
    "        assert num_skips <= 2 * skip_window\n",
    "        self.build_graph()\n",
    "\n",
    "    def predict(self, products):\n",
    "        result = []\n",
    "        for i in range(0, len(products), self.batch_size):\n",
    "            batch = products[i:i+self.batch_size]\n",
    "            batch = self.sess.run(self.gathered, feed_dict={self.train_inputs: batch})\n",
    "            result.append(batch)\n",
    "        return np.concatenate(result, axis=0)\n",
    "\n",
    "    def train(self, num_steps, cv_every_n_steps, cv_steps, lrs):\n",
    "        with ThreadPoolExecutor(max_workers=32) as executor:\n",
    "            average_loss = 0\n",
    "            learning_rate = 1.0\n",
    "            current = executor.submit(self.generate_batch)\n",
    "            for step in range(num_steps):\n",
    "                if step in lrs:\n",
    "                    learning_rate = lrs[step]\n",
    "                batch_inputs, batch_labels = current.result()\n",
    "                current = executor.submit(self.generate_batch)\n",
    "                feed_dict = {self.train_inputs: batch_inputs,\n",
    "                             self.train_labels: batch_labels,\n",
    "                             self.learning_rate: learning_rate}\n",
    "\n",
    "                _, loss_val = self.sess.run([self.optimizer, self.loss], feed_dict=feed_dict)\n",
    "                average_loss += loss_val\n",
    "\n",
    "                if step % 2000 == 0:\n",
    "                    if step > 0:\n",
    "                        average_loss /= 2000\n",
    "                    print('Average loss at step ', step, ': ', average_loss)\n",
    "                    average_loss = 0\n",
    "                if step % cv_every_n_steps == 0:\n",
    "                    self.data = shuffle(self.data, random_state=0)\n",
    "                    self.save_model(step)\n",
    "                    cv_loss = 0\n",
    "                    for batch_inputs, batch_labels in self.generate_test(cv_steps):\n",
    "                        feed_dict = {self.train_inputs: batch_inputs,\n",
    "                                     self.train_labels: batch_labels,\n",
    "                                     self.learning_rate: learning_rate}\n",
    "                        loss_val = self.sess.run(self.loss, feed_dict=feed_dict)\n",
    "                        cv_loss += loss_val\n",
    "                    print('CV',cv_loss / cv_steps)\n",
    "\n",
    "    def save_model(self, step):\n",
    "        self.saver.save(self.sess, os.path.join(self.models_dir, 'prod2vec-skip-gram'), global_step=step)\n",
    "\n",
    "    def load_model(self, path):\n",
    "        self.saver = tf.train.import_meta_graph(self.models_dir + path)\n",
    "        self.saver.restore(self.sess, tf.train.latest_checkpoint(self.models_dir))\n",
    "\n",
    "    def build_graph(self):\n",
    "        self.train_inputs = tf.placeholder(tf.int32, shape=[self.batch_size])\n",
    "        self.train_labels = tf.placeholder(tf.int32, shape=[self.batch_size])\n",
    "        self.learning_rate = tf.placeholder(tf.float32)\n",
    "\n",
    "        # variables\n",
    "        embeddings = tf.Variable(tf.random_uniform([self.vocabulary_size, self.embedding_size], -1.0, 1.0))\n",
    "\n",
    "        softmax_weights = tf.Variable(tf.truncated_normal([self.embedding_size, self.vocabulary_size],\n",
    "                                                          stddev=1.0 / math.sqrt(self.embedding_size)))\n",
    "        softmax_biases = tf.Variable(tf.zeros([self.vocabulary_size]))\n",
    "\n",
    "        self.gathered = tf.gather(embeddings, self.train_inputs)\n",
    "\n",
    "        prediction = tf.matmul(self.gathered, softmax_weights) + softmax_biases\n",
    "        self.loss = tf.reduce_mean(\n",
    "            tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.train_labels, logits=prediction))\n",
    "\n",
    "        self.optimizer = tf.train.GradientDescentOptimizer(self.learning_rate).minimize(self.loss)\n",
    "\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        self.saver = tf.train.Saver()\n",
    "\n",
    "\n",
    "    def inc(self):\n",
    "        self.data_index = (self.data_index + 1) % len(self.data)\n",
    "\n",
    "    def inc_cv(self, data_index):\n",
    "        return (data_index + 1) % len(self.cv_data)\n",
    "\n",
    "    def generate_batch(self):\n",
    "        batch = np.ndarray(shape=(self.batch_size), dtype=np.int32)\n",
    "        labels = np.ndarray(shape=(self.batch_size), dtype=np.int32)\n",
    "        counter = 0\n",
    "        while counter < self.batch_size:\n",
    "            current = self.data.iloc[self.data_index]\n",
    "            if len(current) == 1:\n",
    "                warnings.warn(\"lenght is one\", RuntimeWarning)\n",
    "                self.inc()\n",
    "                continue\n",
    "\n",
    "            span = min(2 * self.skip_window + 1, len(current))\n",
    "\n",
    "            x = target = np.random.randint(0, len(current))\n",
    "\n",
    "            targets_to_avoid = [x]\n",
    "\n",
    "            for j in range(self.num_skips):  # target varies!!! X constant!\n",
    "                while target in targets_to_avoid and len(targets_to_avoid) != span:\n",
    "                    target = np.random.randint(0, span)\n",
    "                if len(targets_to_avoid) == span or counter == self.batch_size:\n",
    "                    break\n",
    "                targets_to_avoid.append(target)\n",
    "                batch[counter] = current[x]\n",
    "                labels[counter] = current[target]\n",
    "                counter += 1\n",
    "            self.inc()\n",
    "\n",
    "        return batch, labels\n",
    "\n",
    "    def generate_test(self, num_steps):\n",
    "        data_index = 0\n",
    "        for _ in range(num_steps):\n",
    "            batch = np.ndarray(shape=(self.batch_size), dtype=np.int32)\n",
    "            labels = np.ndarray(shape=(self.batch_size), dtype=np.int32)\n",
    "\n",
    "            counter = 0\n",
    "            while counter < self.batch_size:\n",
    "                current = self.cv_data.iloc[data_index]\n",
    "                if len(current) == 1:\n",
    "                    warnings.warn(\"lenght is one\", RuntimeWarning)\n",
    "                    data_index = self.inc_cv(data_index)\n",
    "                    continue\n",
    "\n",
    "                span = min(2 * self.skip_window + 1, len(current))\n",
    "\n",
    "                x = target = np.random.randint(0, len(current))\n",
    "\n",
    "                targets_to_avoid = [x]\n",
    "\n",
    "                for j in range(self.num_skips):  # target varies!!! X constant!\n",
    "                    while target in targets_to_avoid and len(targets_to_avoid) != span:\n",
    "                        target = np.random.randint(0, span)\n",
    "                    if len(targets_to_avoid) == span or counter == self.batch_size:\n",
    "                        break\n",
    "                    targets_to_avoid.append(target)\n",
    "                    batch[counter] = current[x]\n",
    "                    labels[counter] = current[target]\n",
    "                    counter += 1\n",
    "                data_index = self.inc_cv(data_index)\n",
    "\n",
    "            yield batch, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define training environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial size 3214874\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "# Data loading\n",
    "np.random.seed(2017)\n",
    "products = pd.read_csv(DATA_DIR + 'products.csv')\n",
    "df = pd.read_pickle(DATA_DIR + 'prod2vec.pkl').products\n",
    "print('initial size', len(df))\n",
    "\n",
    "df_train, df_cv = train_test_split(df, test_size=0.1, random_state=2017)\n",
    "batch_size = 1024\n",
    "rates = {100000: 0.5,\n",
    "         200000: 0.25,\n",
    "         500000: 0.1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model (Prod2Vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "model = Product2VecSkipGram(df_train, df_cv, batch_size, 1, 1, np.max(products.product_id) + 1)\n",
    "model.load_model(os.path.join(MODELS_DIR, 'trained', 'prod2vec-skip-gram-80000.meta'))\n",
    "model.train(120001, 20000, len(df_cv) // batch_size, rates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create embedding file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "model = Product2VecSkipGram(df_train, df_cv, len(products), 1, 1, np.max(products.product_id) + 1)\n",
    "model.load_model(os.path.join(MODELS_DIR, 'trained', 'prod2vec-skip_gram-80000.meta'))\n",
    "embd = model.predict(products.product_id.values)\n",
    "products = pd.concat([products, pd.DataFrame(embd)], axis=1)\n",
    "products.to_pickle(DATA_DIR + 'product_embeddings.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
