{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nuclio - Training function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: ignore\n",
    "import nuclio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%nuclio: setting spec.triggers.retrain.kind to 'cron'\n",
      "%nuclio: setting spec.triggers.retrain.attributes.interval to '1h'\n",
      "%nuclio: setting spec.build.baseImage to 'python:3.6-jessie'\n"
     ]
    }
   ],
   "source": [
    "%%nuclio config\n",
    "\n",
    "# Trigger\n",
    "spec.triggers.retrain.kind = \"cron\"\n",
    "spec.triggers.retrain.attributes.interval = \"1h\"\n",
    "\n",
    "# Base image\n",
    "spec.build.baseImage = \"python:3.6-jessie\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%nuclio: setting 'FROM_TSDB' environment variable\n",
      "%nuclio: setting 'V3IO_FRAMESD' environment variable\n",
      "%nuclio: setting 'V3IO_USERNAME' environment variable\n",
      "%nuclio: setting 'V3IO_ACCESS_KEY' environment variable\n",
      "%nuclio: setting 'FEATURES_TABLE' environment variable\n",
      "%nuclio: setting '# FEATURES_TABLE' environment variable\n",
      "%nuclio: setting 'PREDICTIONS_TABLE' environment variable\n",
      "%nuclio: setting '# PREDICTIONS_TABLE' environment variable\n",
      "%nuclio: setting 'TRAIN_ON_LAST' environment variable\n",
      "%nuclio: setting 'TRAIN_SIZE' environment variable\n",
      "%nuclio: setting 'NUMBER_OF_SHARDS' environment variable\n",
      "%nuclio: setting '# MODEL_FILENAME' environment variable\n",
      "%nuclio: setting 'MODEL_FILENAME' environment variable\n",
      "%nuclio: setting 'SOURCE_MODEL_DIR' environment variable\n",
      "%nuclio: setting 'FIXED_WEB_DIR' environment variable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "%nuclio: cannot find \"=\" in line\n",
      "%nuclio: cannot find \"=\" in line\n",
      "%nuclio: cannot find \"=\" in line\n",
      "%nuclio: cannot find \"=\" in line\n",
      "%nuclio: cannot find \"=\" in line\n",
      "%nuclio: cannot find \"=\" in line\n",
      "%nuclio: cannot find \"=\" in line\n"
     ]
    }
   ],
   "source": [
    "%%nuclio env\n",
    "\n",
    "# Work from TSDB or Parquet?\n",
    "FROM_TSDB=1\n",
    "\n",
    "# DB Config\n",
    "V3IO_FRAMESD=${V3IO_FRAMESD}\n",
    "V3IO_USERNAME=${V3IO_USERNAME}\n",
    "V3IO_ACCESS_KEY=${V3IO_ACCESS_KEY}\n",
    "\n",
    "# Features\n",
    "FEATURES_TABLE=netops_features\n",
    "# FEATURES_TABLE=/v3io/bigdata/netops_features_parquet\n",
    "\n",
    "# Predictions\n",
    "PREDICTIONS_TABLE=netops_predictions\n",
    "# PREDICTIONS_TABLE=/v3io/bigdata/netops_predictions_parquet\n",
    "\n",
    "# Training\n",
    "TRAIN_ON_LAST=1d\n",
    "TRAIN_SIZE=0.7\n",
    "\n",
    "# Parallelizem\n",
    "NUMBER_OF_SHARDS=4\n",
    "\n",
    "# Model\n",
    "# MODEL_FILENAME=netops.model.pickle\n",
    "MODEL_FILENAME=netops.v3.model.pickle\n",
    "SOURCE_MODEL_DIR=/bigdata/netops/models\n",
    "FIXED_WEB_DIR=/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%nuclio: setting 'CURRENT_MODEL_DIR' environment variable\n"
     ]
    }
   ],
   "source": [
    "%nuclio env -c CURRENT_MODEL_DIR=/models\n",
    "%nuclio env -l CURRENT_MODEL_DIR=/v3io/bigdata/netops/models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarrow in /User/.pythonlibs/lib/python3.6/site-packages (0.13.0)\n",
      "Requirement already satisfied: six>=1.0.0 in /conda/lib/python3.6/site-packages (from pyarrow) (1.12.0)\n",
      "Requirement already satisfied: numpy>=1.14 in /conda/lib/python3.6/site-packages (from pyarrow) (1.16.4)\n",
      "Requirement already satisfied: pandas in /conda/lib/python3.6/site-packages (0.23.4)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /conda/lib/python3.6/site-packages (from pandas) (2.8.0)\n",
      "Requirement already satisfied: pytz>=2011k in /conda/lib/python3.6/site-packages (from pandas) (2019.1)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /conda/lib/python3.6/site-packages (from pandas) (1.16.4)\n",
      "Requirement already satisfied: six>=1.5 in /conda/lib/python3.6/site-packages (from python-dateutil>=2.5.0->pandas) (1.12.0)\n",
      "Requirement already up-to-date: v3io_frames in /User/.pythonlibs/lib/python3.6/site-packages (0.5.6)\n",
      "Requirement already satisfied, skipping upgrade: pandas==0.23.* in /conda/lib/python3.6/site-packages (from v3io_frames) (0.23.4)\n",
      "Requirement already satisfied, skipping upgrade: requests>=2.19.1 in /conda/lib/python3.6/site-packages (from v3io_frames) (2.22.0)\n",
      "Requirement already satisfied, skipping upgrade: googleapis-common-protos>=1.5.3 in /conda/lib/python3.6/site-packages (from v3io_frames) (1.6.0)\n",
      "Requirement already satisfied, skipping upgrade: grpcio-tools>=1.16.0 in /conda/lib/python3.6/site-packages (from v3io_frames) (1.21.1)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.5.0 in /conda/lib/python3.6/site-packages (from pandas==0.23.*->v3io_frames) (2.8.0)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2011k in /conda/lib/python3.6/site-packages (from pandas==0.23.*->v3io_frames) (2019.1)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.9.0 in /conda/lib/python3.6/site-packages (from pandas==0.23.*->v3io_frames) (1.16.4)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /conda/lib/python3.6/site-packages (from requests>=2.19.1->v3io_frames) (2019.3.9)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /conda/lib/python3.6/site-packages (from requests>=2.19.1->v3io_frames) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /conda/lib/python3.6/site-packages (from requests>=2.19.1->v3io_frames) (1.24.2)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /conda/lib/python3.6/site-packages (from requests>=2.19.1->v3io_frames) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.6.0 in /conda/lib/python3.6/site-packages (from googleapis-common-protos>=1.5.3->v3io_frames) (3.8.0)\n",
      "Requirement already satisfied, skipping upgrade: grpcio>=1.21.1 in /conda/lib/python3.6/site-packages (from grpcio-tools>=1.16.0->v3io_frames) (1.21.1)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5 in /conda/lib/python3.6/site-packages (from python-dateutil>=2.5.0->pandas==0.23.*->v3io_frames) (1.12.0)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /conda/lib/python3.6/site-packages (from protobuf>=3.6.0->googleapis-common-protos>=1.5.3->v3io_frames) (41.0.1)\n",
      "Requirement already satisfied: xgboost in /User/.pythonlibs/lib/python3.6/site-packages (0.90)\n",
      "Requirement already satisfied: numpy in /conda/lib/python3.6/site-packages (from xgboost) (1.16.4)\n",
      "Requirement already satisfied: scipy in /conda/lib/python3.6/site-packages (from xgboost) (1.2.1)\n",
      "Requirement already satisfied: scikit-learn==0.20.1 in /User/.pythonlibs/lib/python3.6/site-packages (0.20.1)\n",
      "Requirement already satisfied: numpy>=1.8.2 in /conda/lib/python3.6/site-packages (from scikit-learn==0.20.1) (1.16.4)\n",
      "Requirement already satisfied: scipy>=0.13.3 in /conda/lib/python3.6/site-packages (from scikit-learn==0.20.1) (1.2.1)\n",
      "Reading package lists... Done\n",
      "E: List directory /var/lib/apt/lists/partial is missing. - Acquire (13: Permission denied)\n",
      "mkdir: cannot create directory ‘/models’: Permission denied\n"
     ]
    }
   ],
   "source": [
    "%%nuclio cmd\n",
    "\n",
    "############\n",
    "# installs #\n",
    "############\n",
    "\n",
    "# Utils\n",
    "pip install pyarrow\n",
    "pip install pandas\n",
    "\n",
    "# Igz DB\n",
    "pip install v3io_frames --upgrade\n",
    "\n",
    "# Function\n",
    "pip install xgboost\n",
    "pip install scikit-learn==0.20.1\n",
    "\n",
    "apt-get update && apt-get install -y wget\n",
    "mkdir -p ${FIXED_WEB_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%nuclio cmd -c \n",
    "# Copy the model file into the function\n",
    "wget -O ${FIXED_WEB_DIR}/${MODEL_FILENAME} --header \"x-v3io-session-key: ${V3IO_ACCESS_KEY}\" http://${V3IO_WEBAPI_SERVICE_HOST}:8081${SOURCE_MODEL_DIR}/${MODEL_FILENAME}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import v3io_frames as v3f\n",
    "\n",
    "import pandas as pd\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_df_from_tsdb(context, df):\n",
    "    df.index.names = ['timestamp', 'company', 'data_center', 'device']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_indexes(df):\n",
    "    df = df.set_index(['timestamp', 'company', 'data_center', 'device'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_tsdb(context):\n",
    "    df = context.v3f.read(backend='tsdb', query=f'select * from {context.features_table}',\n",
    "                          start=f'now-{context.train_on_last}', end='now', multi_index=True)\n",
    "    df = format_df_from_tsdb(context, df)\n",
    "    \n",
    "    # Keep columns\n",
    "    keep_columns = [col for col in df.columns if 'is_error' not in col]\n",
    "    \n",
    "    # Keep good columns and Sort them\n",
    "    df = df[sorted(keep_columns)]\n",
    "    print(df.columns)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_parquet(context):\n",
    "    # Get parquet files\n",
    "    mpath = [os.path.join(context.features_table, file) for file in os.listdir(context.features_table)]\n",
    "    \n",
    "    # Get latest filename\n",
    "    latest = max(mpath, key=os.path.getmtime)\n",
    "    print(latest)\n",
    "    context.logger.debug(f'Reading data from: {latest}')\n",
    "    \n",
    "    # Load parquet to dask\n",
    "    df = pd.read_parquet(latest)\n",
    "    \n",
    "    # Keep columns\n",
    "    keep_columns = [col for col in df.columns if 'is_error' not in col]\n",
    "    \n",
    "    # Keep good columns and Sort them\n",
    "    df = df[sorted(keep_columns)]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_tsdb(context, df: pd.DataFrame):   \n",
    "    # Fix indexes before saving to TSDB\n",
    "    df = set_indexes(df)\n",
    "    \n",
    "    # Save to TSDB\n",
    "    context.v3f.write('tsdb', context.predictions_table, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_parquet(context, df: pd.DataFrame):\n",
    "    print('Saving features to Parquet')\n",
    "    \n",
    "    # Need to fix timestamps from ns to ms if we write to parquet\n",
    "    df = df.reset_index()\n",
    "    df['timestamp'] = df.loc[:, 'timestamp'].astype('datetime64[ms]')\n",
    "    \n",
    "    # Fix indexes\n",
    "    df= set_indexes(df)\n",
    "    \n",
    "    # Save parquet\n",
    "    first_timestamp = df.index[0][0].strftime('%Y%m%dT%H%M%S')\n",
    "    last_timestamp = df.index[-1][0].strftime('%Y%m%dT%H%M%S')\n",
    "    filename = first_timestamp + '-' + last_timestamp + '.parquet'\n",
    "    filepath = os.path.join(context.predictions_table, filename)\n",
    "    with open(filepath, 'wb+') as f:\n",
    "        df.to_parquet(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_context(context):\n",
    "    \n",
    "    # Save features directory\n",
    "    features_table = os.getenv('FEATURES_TABLE', 'netops_features')\n",
    "    setattr(context, 'features_table', features_table)\n",
    "    \n",
    "    # Save predictions directory\n",
    "    predictions_table = os.getenv('PREDICTIONS_TABLE', 'netops_predictions')\n",
    "    setattr(context, 'predictions_table', predictions_table)\n",
    "    \n",
    "    # Get saving configuration\n",
    "    is_from_tsdb = (int(os.getenv('FROM_TSDB', 1)) == 1)\n",
    "    \n",
    "    # Save to TSDB\n",
    "    if is_from_tsdb:\n",
    "        # Create our DB client\n",
    "        v3io_client = v3f.Client(address='framesd:8081', container='bigdata')\n",
    "        setattr(context, 'v3f', v3io_client)\n",
    "        \n",
    "        # Create predictions table if neede\n",
    "        context.v3f.create('tsdb', context.predictions_table, attrs={'rate': '1/s'}, if_exists=1)\n",
    "        \n",
    "        train_on_last = os.getenv('TRAIN_ON_LAST', '1h')\n",
    "        setattr(context, 'train_on_last', train_on_last)\n",
    "        \n",
    "        # Set TSDB reading function\n",
    "        setattr(context, 'read', get_data_tsdb)\n",
    "        \n",
    "        # Set TSDB saving function\n",
    "        setattr(context, 'write', save_to_tsdb)\n",
    "        \n",
    "    # Save to Parquet\n",
    "    else:\n",
    "         # Create saving directory if needed\n",
    "        filepath = os.path.join(context.predictions_table)\n",
    "        if not os.path.exists(filepath):\n",
    "            os.makedirs(filepath)\n",
    "            \n",
    "        # Set Parquet reading function\n",
    "        setattr(context, 'read', get_data_parquet)\n",
    "        \n",
    "        # Set Parquet saving function\n",
    "        setattr(context, 'write', save_to_parquet)\n",
    "    \n",
    "    # Load the model\n",
    "    model_path = os.path.join(os.getenv('CURRENT_MODEL_DIR', '/models'), os.getenv('MODEL_FILENAME', 'netops.v1.model'))\n",
    "    with open(model_path, 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    setattr(context, 'model', model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handler(context, event):\n",
    "\n",
    "    # Load last hour data\n",
    "    df = context.read(context)\n",
    "    \n",
    "    # limit for testing\n",
    "    df = df.head(2)\n",
    "    \n",
    "    # Predict\n",
    "    df['prediction'] = context.model.predict(df.values)\n",
    "    \n",
    "    print(df.head(1))\n",
    "    \n",
    "    # Prepare to save predictions\n",
    "    df = df.reset_index()\n",
    "    df = df.rename({'level_0': 'time',\n",
    "                    'level_1': 'company',\n",
    "                    'level_2': 'data_center',\n",
    "                    'level_3': 'device'}, axis=1)\n",
    "    \n",
    "    # Save\n",
    "    context.write(context, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: ignore\n",
    "init_context(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['cpu_utilization', 'cpu_utilization_hourly', 'cpu_utilization_minutely',\n",
      "       'latency', 'latency_hourly', 'latency_minutely', 'packet_loss',\n",
      "       'packet_loss_hourly', 'packet_loss_minutely', 'throughput',\n",
      "       'throughput_hourly', 'throughput_minutely'],\n",
      "      dtype='object')\n",
      "                                                                      cpu_utilization  \\\n",
      "timestamp               company          data_center   device                           \n",
      "2019-06-20 14:58:24.605 Johnson_and_Sons Hoffman_Trace 0405787217462        72.854064   \n",
      "\n",
      "                                                                      cpu_utilization_hourly  \\\n",
      "timestamp               company          data_center   device                                  \n",
      "2019-06-20 14:58:24.605 Johnson_and_Sons Hoffman_Trace 0405787217462               76.420022   \n",
      "\n",
      "                                                                      cpu_utilization_minutely  \\\n",
      "timestamp               company          data_center   device                                    \n",
      "2019-06-20 14:58:24.605 Johnson_and_Sons Hoffman_Trace 0405787217462                 71.008158   \n",
      "\n",
      "                                                                      latency  \\\n",
      "timestamp               company          data_center   device                   \n",
      "2019-06-20 14:58:24.605 Johnson_and_Sons Hoffman_Trace 0405787217462  4.47977   \n",
      "\n",
      "                                                                      latency_hourly  \\\n",
      "timestamp               company          data_center   device                          \n",
      "2019-06-20 14:58:24.605 Johnson_and_Sons Hoffman_Trace 0405787217462       19.602218   \n",
      "\n",
      "                                                                      latency_minutely  \\\n",
      "timestamp               company          data_center   device                            \n",
      "2019-06-20 14:58:24.605 Johnson_and_Sons Hoffman_Trace 0405787217462          1.493257   \n",
      "\n",
      "                                                                      packet_loss  \\\n",
      "timestamp               company          data_center   device                       \n",
      "2019-06-20 14:58:24.605 Johnson_and_Sons Hoffman_Trace 0405787217462      0.62291   \n",
      "\n",
      "                                                                      packet_loss_hourly  \\\n",
      "timestamp               company          data_center   device                              \n",
      "2019-06-20 14:58:24.605 Johnson_and_Sons Hoffman_Trace 0405787217462            8.787566   \n",
      "\n",
      "                                                                      packet_loss_minutely  \\\n",
      "timestamp               company          data_center   device                                \n",
      "2019-06-20 14:58:24.605 Johnson_and_Sons Hoffman_Trace 0405787217462              0.834527   \n",
      "\n",
      "                                                                      throughput  \\\n",
      "timestamp               company          data_center   device                      \n",
      "2019-06-20 14:58:24.605 Johnson_and_Sons Hoffman_Trace 0405787217462  273.852044   \n",
      "\n",
      "                                                                      throughput_hourly  \\\n",
      "timestamp               company          data_center   device                             \n",
      "2019-06-20 14:58:24.605 Johnson_and_Sons Hoffman_Trace 0405787217462         210.776314   \n",
      "\n",
      "                                                                      throughput_minutely  \\\n",
      "timestamp               company          data_center   device                               \n",
      "2019-06-20 14:58:24.605 Johnson_and_Sons Hoffman_Trace 0405787217462           273.986271   \n",
      "\n",
      "                                                                      prediction  \n",
      "timestamp               company          data_center   device                     \n",
      "2019-06-20 14:58:24.605 Johnson_and_Sons Hoffman_Trace 0405787217462         0.0  \n"
     ]
    }
   ],
   "source": [
    "# nuclio: ignore\n",
    "event = nuclio.Event(body='')\n",
    "output = handler(context, event)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nuclio.deploy] 2019-06-20 14:21:34,851 (info) Building processor image\n",
      "[nuclio.deploy] 2019-06-20 14:22:11,183 (info) Pushing image\n",
      "[nuclio.deploy] 2019-06-20 14:22:37,406 (info) Build complete\n",
      "[nuclio.deploy] 2019-06-20 14:22:41,440 (info) Function deploy complete\n",
      "[nuclio.deploy] 2019-06-20 14:22:41,446 done creating predict, function address: 18.185.111.133:32161\n",
      "%nuclio: function deployed\n"
     ]
    }
   ],
   "source": [
    "%nuclio deploy -p netops -n predict -c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
