# End-to-End Platform Use-Case Application Demos

**In This Document**

- [Overview](#overview)
- [Image Classification](#image-classification-demo)
- [Predictive Infrastructure Monitoring](#netops-demo)
- [Natural Language Processing (NLP)](#nlp-demo)
- [Stream Enrichment](#stream-enrich-demo)
- [Stock Trading](#stocks-demo)

<a id="overview"></a>
## Overview

The **demos** tutorials directory contains full end-to-end use-case applications that demonstrate how to use the Iguazio Data Science Platform ("the platform") and related tools to address data science requirements for different industries and implementations.

<a id="image-classification-demo"></a>
## Image Classification

The [**image-classification**](image-classification/01-image-classification.ipynb) demo demonstrates an end-to-end solution for image recognition: the application uses TensorFlow, Keras, Horovod, and Nuclio to build and train an ML model that identifies (recognizes) and classifies images. 
The application consists of four MLRun and Nuclio functions for performing the following operations:

1. Import an image archive from from an Amazon Simple Storage (S3) bucket to the platform's data store.
2. Tag the images based on their name structure.
3. Train the image-classification ML model by using [TensorFlow](https://www.tensorflow.org/) and [Keras](https://keras.io/); use [Horovod](https://eng.uber.com/horovod/) to perform distributed training over either GPUs or CPUs.
4. Automatically deploy a Nuclio model-serving function from [Jupyter Notebook](nuclio-serving-tf-images.ipynb) or from a [Dockerfile](./inference-docker).

This demo also provides an example of an [automated pipeline](image-classification/02-create_pipeline.ipynb) using [MLRun](https://github.com/mlrun/mlrun) and [Kubeflow pipelines](https://github.com/kubeflow/pipelines).

<a id="netops-demo"></a>
## Predictive Infrastructure Monitoring

The [**netops**](netops/01-generator.ipynb) demo demonstrates predictive infrastructure monitoring: the application builds, trains, and deploys a machine-learning model for analyzing and predicting failure in network devices as part of a network operations (NetOps) flow.
The goal is to identify anomalies for device metrics &mdash; such as CPU, memory consumption, or temperature &mdash; which can signify an upcoming issue or failure.

- The model training is sped up by using the [Dask](https://dask.org/) Python library for parallel computing, which extends [pandas](https://pandas.pydata.org/) DataFrames.
- The model prediction is done by using open-source Python libraries &mdash; including [scikit-learn](https://scikit-learn.org) (a.k.a. sklearn), [SciPy](https://www.scipy.org/scipylib/), and [NumPy](http://www.numpy.org/) &mdash; and the gradient boosting ML technique.
- The data is generated by using an open-source generator tool that was written by Iguazio.
  This generator enables users to customize the metrics, data range, and many other parameters, and prepare a data set that's suitable for other similar use cases.

<a id="nlp-demo"></a>
## Natural Language Processing (NLP)

The [**nlp**](nlp/nlp-example.ipynb) demo demonstrates natural language processing (NLP): the application processes natural-language textual data &mdash; including spelling correction and sentiment analysis &mdash; and generates a Nuclio serverless function that translates any given text string to another (configurable) language.

- The textual data is collected and processed by using the [TextBlob](https://textblob.readthedocs.io/) Python NLP library. The processing includes spelling correction and sentiment analysis.
- A serverless function that translates text to another language, which is configured in an environment variable, is generated by using the [Nuclio](https://nuclio.io/) framework.

<a id="stocks-demo"></a>
## Smart Stock Trading

The [**stocks**](stocks/01-gen-demo-data.ipynb) demo demonstrates a smart stock-trading application: 
the application reads stock-exchange data from an internet service into a time-series database (TSDB); uses Twitter to analyze the market sentiment on specific stocks, in real time; and saves the data to a platform NoSQL table that is used for generating reports and analyzing and visualizing the data on a Grafana dashboard.

- The stock data is read from Twitter by using the [TwythonStreamer](https://twython.readthedocs.io/en/latest/usage/streaming_api.html) Python wrapper to the Twitter Streaming API, and saved to TSDB and NoSQL tables in the platform.
- Sentiment analysis is done by using the [TextBlob](https://textblob.readthedocs.io/) Python library for natural language processing (NLP).
- The analyzed data is visualized as graphs on a [Grafana](https://grafana.com/grafana) dashboard, which is created from the Jupyter notebook code.
  The data is read from both the TSDB and NoSQL stock tables.

<a id="stream-enrich-demo"></a>
### Stream Enrichment

The [**stream-enrich**](stream-enrich/stream-enrich.ipynb) demo demonstrates a typical stream-based data-engineering pipeline, which is required in many real-world scenarios: data is streamed from an event streaming engine; the data is enriched, in real time, using data from a NoSQL table; the enriched data is saved to an output data stream and then consumed from this stream.

- Car-owner data is streamed into the platform from a simulated streaming engine by using an event-triggered [Nuclio](https://nuclio.io/) serverless function.
- The data is written (ingested) into an input platform stream by using the the platform's [Streaming Web API](https://www.iguazio.com/docs/reference/latest-release/api-reference/web-apis/streaming-web-api/).
- The input stream data is enriched with additional data, such as the car's color and vendor, and the enriched data is saved to a NoSQL table by using the platform's [NoSQL Web API](https://www.iguazio.com/docs/reference/latest-release/api-reference/web-apis/nosql-web-api/).
- The Nuclio function writes the enriched data to an output platform data stream by using the platform's Streaming Web API.
- The enriched data is read (consumed) from the output stream by using the platform's Streaming Web API.
