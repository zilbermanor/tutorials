{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standalone Python Function for Unified Data Batching and Aggregation with RAPIDS cuDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kafka\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/21/71/73286e748ac5045b6a669c2fe44b03ac4c5d3d2af9291c4c6fc76438a9a9/kafka-1.3.5-py2.py3-none-any.whl (207kB)\n",
      "\u001b[K    100% |████████████████████████████████| 215kB 20.1MB/s ta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: kafka\n",
      "Successfully installed kafka-1.3.5\n"
     ]
    }
   ],
   "source": [
    "!pip install kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer\n",
    "import os\n",
    "import glob\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import itertools\n",
    "import json\n",
    "\n",
    "# Select the DataFrame factory\n",
    "import cudf as pd\n",
    "# Import pandas as pd\n",
    "\n",
    "# Basic configuration\n",
    "metric_names = ['cpu_utilization', 'latency', 'packet_loss', 'throughput']\n",
    "batch_len = 100\n",
    "batch = list()\n",
    "\n",
    "# Kafka configuration\n",
    "topic = ''\n",
    "servers = []\n",
    "offset = 'earliest'\n",
    "\n",
    "def handler(event):\n",
    "    '''\n",
    "        Processing function\n",
    "    '''\n",
    "    global batch\n",
    "    global metric_names\n",
    "    \n",
    "    # Aggregate event JSON objects\n",
    "    batch.append(event.body)\n",
    "    \n",
    "    # Verify that there are enough events to perform aggregations\n",
    "    if len(batch) >= interval:\n",
    "        \n",
    "        # Create a DataFrame from the batch of event JSON objects\n",
    "        df = cudf.read_json('\\n'.join(batch), lines=True)\n",
    "        df = df.reset_index(drop=True)\n",
    "        \n",
    "        # Perform aggregations\n",
    "        df = df.groupby(['company']).\\\n",
    "                    agg({k: ['min', 'max', 'mean'] for k in metric_names})\n",
    "        \n",
    "        # Save to Parquet\n",
    "        filename = f'{time.time()}.parquet'\n",
    "        filepath = os.path.join(sink, filename)\n",
    "        new_index = [f'{e[0]}_{e[1]}' for e in list(df.columns)]\n",
    "        df.columns = new_index\n",
    "        df.to_parquet(filepath)\n",
    "        \n",
    "        # Reset the batch\n",
    "        batch = list()\n",
    "\n",
    "# Kafka handling\n",
    "consumer = KafkaConsumer(\n",
    "     topic,\n",
    "     bootstrap_servers=servers,\n",
    "     auto_offset_reset='offset',\n",
    "     value_deserializer=lambda x: x.decode('utf-8'))\n",
    "\n",
    "for message in consumer:\n",
    "    message = message.value\n",
    "    handler(message)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
